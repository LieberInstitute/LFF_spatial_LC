---
title: "03b-sctransform_dimreduction_testing"
output: html_document
date: "2024-08-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.height = 10,fig.width = 7,include = FALSE)
#### sets tab autocompletion for directories to begin from the directory containing the .Rproj session, instead of the script's directory if different
knitr::opts_knit$set(root.dir = here::here())

library(data.table)
library(SpatialExperiment)
library(scry)
library(glmpca)
library(HDF5Array)
# library(Seurat)
# library(sctransform)

## rstudio GUI tweaks
require(colorout)
ColorOut()
options("styler.addins_style_transformer" = "biocthis::bioc_style()")
##

## enable forked parallel processing with BiocParallel::multicoreParam, future::, etc. seemed to need this a couple times, but otherwise havent so its here as a preventative measure. part of this is adding the line 
# OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES
# to Renviron.site. see e.g. top response on https://stackoverflow.com/questions/73638290/python-on-mac-is-it-safe-to-set-objc-disable-initialize-fork-safety-yes-globall 
options(parallelly.supportsMulticore.disableOn="")
options(parallelly.fork.enable=TRUE)
library(parallelly)
options(bphost="localhost")
library(BiocParallel)

## last of all, unload the base package datasets, whose data keep getting in the way of autocompletions (e.g., Theoph is priortized over TRUE)
unloadNamespace("datasets")
```

```{r}


```

# one time run: ### load the SPE, create an HDF5Array object from the raw counts
```{r}
# lc <- readRDS("processed-data/05_QC/04-lffLC_noEdges_noLocOutliers_remainder1pctileUMIorGene-removed.RDS")

## write the counts out to an HDF5Array
## per https://support.bioconductor.org/p/107468/ this can be sped up 3x just by specifying the HDF5 dump file
## as will this, assuming RAM availability (this was 6 years ago, so I'm sure its fine), vs. default value of 1e6. allowable max is 4.5e8
setHDF5DumpChunkLength(length = 5e7)

# setHDF5DumpFile("processed-data/05_QC/04b_lffLC_noEdges_noLocOutliers_remainder1pctileUMIorGene-removed_counts.h5")
# writeHDF5Array(Matrix::Matrix(counts(lc)),name="counts",with.dimnames=T)
# rm(lc)
# gc(full=T)
```

```{r}
lcct <- HDF5Array("processed-data/05_QC/04b_lffLC_noEdges_noLocOutliers_remainder1pctileUMIorGene-removed_binomdevs.h5",name="counts",as.sparse = T)
```

```{r}
# initialize the file we'll write the output to
setHDF5DumpFile("processed-data/05_QC/04b_lffLC_noEdges_noLocOutliers_remainder1pctileUMIorGene-removed_nullresiduals.h5")

nullresid <- nullResiduals(lcct)

writeHDF5Array(nullresid,name="nullresid",with.dimnames=T)




```

scTransform-no dice
```{r}
# library(future) # for seurat parallelization
# options('future.globals.maxSize'=42000*1024^2) ## 8000*1024^2 bytes = 8GB

# lc2 <- Seurat::CreateSeuratObject(meta.data = as.data.frame(colData(lc)),counts = counts(lc),assay = "counts",min.cells = 1,min.features = 1)
# 
# rm(lc)
# gc(full=T)

# plan("cluster",workers=3)
# lc2 <- SCTransform(lc2,assay = "counts",ncells = 15000,variable.features.n = 6000,conserve.memory = T,vars.to.regress = "sample_id")
# Running SCTransform on assay: counts
# Running SCTransform on layer: counts
# nope, way outta memory
```

